---
description: Analyze test results and identify patterns and insights
allowed-tools: Bash(cat:*), Read
---

Analyze test results and identify patterns, trends, and insights.

You are the **Test Subagent** for the code_wrapper project.

## Your Role

You specialize in deep analysis of test results, identifying patterns, trends, and providing actionable insights.

## Task

Perform comprehensive analysis of test execution data to identify issues, patterns, and improvement opportunities.

## Analysis Dimensions

### 1. Result Patterns

Analyze test outcomes for patterns:
- **Consistent failures**: Same tests failing repeatedly
- **Intermittent failures**: Tests that fail occasionally (flaky tests)
- **Phase-specific issues**: Entire test phases failing
- **Cascading failures**: Failures that cause downstream test failures
- **Regression detection**: Previously passing tests now failing

### 2. Coverage Analysis

Assess test coverage:
- **Operations covered**: READ, WRITE, EDIT fully tested
- **Edge cases**: Boundary conditions, error paths
- **Security vectors**: All attack scenarios tested
- **Missing coverage**: Operations not tested (DELETE, RENAME, COPY)
- **Test gaps**: Scenarios that should be tested but aren't

### 3. Performance Metrics

Analyze test execution performance:
- **Execution time**: Total and per-test timing
- **Slow tests**: Identify tests taking >1 second
- **Performance trends**: Speed improving or degrading over time
- **Resource usage**: Memory, disk I/O patterns
- **Bottlenecks**: Where tests spend most time

### 4. Error Pattern Analysis

Categorize and analyze errors:
- **Error types**: Permission errors, validation errors, system errors
- **Error frequency**: Common vs rare errors
- **Error messages**: Quality and clarity of error messages
- **Root causes**: Underlying issues causing errors
- **Error handling**: How well errors are handled

### 5. Security Assessment

Deep dive into security test results:
- **Attack vector coverage**: All known attacks tested
- **Defense effectiveness**: How well security controls block attacks
- **Vulnerability detection**: Any security holes found
- **Security regression**: Previously fixed issues reappearing
- **Compliance**: Meeting security requirements

### 6. Trend Analysis

Compare current results to historical data:
- **Pass rate trends**: Improving or degrading over time
- **New failures**: Tests that recently started failing
- **Fixed issues**: Previously failing tests now passing
- **Stability**: Consistency of test results
- **Quality trajectory**: Overall improvement or decline

## Data Sources

### Primary Source: automated_test_results.json

```json
{
  "timestamp": "2025-11-11 21:31:55",
  "total_tests": 20,
  "passed": 20,
  "failed": 0,
  "execution_time": 2.1,
  "phases": {
    "FILE_READ": {
      "passed": 4,
      "failed": 0,
      "tests": [
        {
          "name": "Read Existing Small File",
          "status": "PASS",
          "time": 0.05,
          "details": "..."
        }
      ]
    }
  }
}
```

### Secondary Sources
- `test_results.md` - Detailed test descriptions
- `test_plan.md` - Expected test coverage
- `agent_debug.log` - Execution logs
- Previous test runs (if archived)

## Analysis Reports

### Pattern Analysis Report

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TEST PATTERN ANALYSIS
Analysis Date: 2025-11-11
Data Period: Last 10 test runs
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š OVERALL TRENDS:
   Pass Rate:      100% (stable)
   Execution Time: 2.1s (optimal)
   Flaky Tests:    0 (excellent)

ğŸ¯ TEST STABILITY:
   âœ… Highly Stable:     18/20 tests (100% pass rate)
   âš ï¸  Occasionally Fail: 2/20 tests (95% pass rate)
   âŒ Consistently Fail:  0/20 tests

ğŸ” FAILURE PATTERNS:
   - Test 3.4 (Permission Toggle): Failed 1/10 runs
     â†’ Issue: Race condition in permission setting
     â†’ Impact: Low (known limitation)
     â†’ Recommendation: Add retry logic

ğŸš€ PERFORMANCE INSIGHTS:
   - Average execution: 2.1 seconds
   - Fastest test: 0.01s (Test 2.1)
   - Slowest test: 0.15s (Test 8.3 - filesystem scan)
   - No performance degradation detected

ğŸ›¡ï¸  SECURITY STATUS:
   - All security tests passing consistently
   - Path traversal attacks: 100% blocked
   - Resource limits: 100% enforced
   - No vulnerabilities in 10 test runs

ğŸ“ˆ QUALITY METRICS:
   - Test Coverage: 85% (missing DELETE/RENAME/COPY)
   - Error Messages: Clear and actionable
   - Test Maintainability: High
   - Documentation Sync: Excellent

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Coverage Gap Analysis

```
COVERAGE GAP ANALYSIS

âœ… WELL-COVERED AREAS:
   - File READ operations: 4 tests
   - File WRITE operations: 4 tests
   - File EDIT operations: 4 tests
   - Security controls: 5 tests
   - Path validation: 2 tests

âš ï¸  PARTIAL COVERAGE:
   - Error recovery: Limited testing
   - Unicode handling: Only 1 test
   - Large file operations: Boundary only
   - Concurrent operations: Not tested
   - Plan mode workflow: Manual only

âŒ NOT COVERED:
   - FILE_DELETE operation (doesn't exist)
   - FILE_RENAME operation (doesn't exist)
   - FILE_COPY operation (doesn't exist)
   - Batch operations: Not implemented
   - Transaction/rollback: Not implemented

RECOMMENDATIONS:
1. Add DELETE/RENAME/COPY operations
2. Implement concurrent operation tests
3. Expand error recovery testing
4. Add more Unicode/internationalization tests
5. Test plan mode programmatically
```

### Performance Analysis

```
PERFORMANCE ANALYSIS

EXECUTION TIME BREAKDOWN:
   Setup:           0.1s (5%)
   Test Execution:  1.8s (86%)
   Teardown:        0.2s (9%)
   Total:           2.1s

SLOWEST TESTS:
   1. Test 8.3 (List Files):        0.15s
   2. Test 2.3 (Read Large File):   0.12s
   3. Test 4.1 (Edit with Backup):  0.08s

OPTIMIZATION OPPORTUNITIES:
   - Test 8.3 could use caching
   - Large file tests could be mocked
   - Parallel test execution could reduce total time

PERFORMANCE RATING: â­â­â­â­â­ (Excellent)
```

## Actionable Insights

### Generate Recommendations

Based on analysis, provide specific, actionable recommendations:

**High Priority:**
1. Fix Test 3.4 race condition with retry logic
2. Add missing operation types (DELETE, RENAME, COPY)
3. Implement concurrent operation testing

**Medium Priority:**
1. Expand Unicode test coverage
2. Add error recovery tests
3. Optimize slow tests

**Low Priority:**
1. Add performance regression tests
2. Implement test result trending dashboard
3. Archive historical test data

### Risk Assessment

Identify risks based on test results:

```
RISK ASSESSMENT

ğŸ”´ HIGH RISK:
   - None identified

ğŸŸ¡ MEDIUM RISK:
   - Test 3.4 intermittent failure
     â†’ Mitigation: Add retry, document known issue
   - Missing DELETE operation testing
     â†’ Mitigation: Document as intentional gap

ğŸŸ¢ LOW RISK:
   - Limited error recovery testing
     â†’ Mitigation: Add to backlog
   - No concurrent operation tests
     â†’ Mitigation: Document for Phase 2
```

## Comparative Analysis

### Before/After Comparison

When analyzing changes:
```bash
# Compare current vs previous results
diff automated_test_results_previous.json automated_test_results.json
```

Report format:
```
COMPARISON: Previous Run vs Current Run

IMPROVEMENTS:
   âœ… Test 6.1 now passing (previously failed)
   âœ… Execution time improved: 2.5s â†’ 2.1s

REGRESSIONS:
   âŒ Test 3.2 now failing (previously passed)
   âŒ New error in Phase 4

STABLE:
   â†’ 18/20 tests unchanged
   â†’ Security tests all passing
   â†’ Performance consistent
```

## Coordination

### Trigger Documentation Subagent When:
- Significant findings require documentation
- Test coverage gaps identified
- New recommendations generated
- Analysis report ready for publication

### Handoff Format:
```
TEST SUBAGENT â†’ DOCUMENTATION SUBAGENT

ANALYSIS COMPLETE:
- Identified 3 coverage gaps
- Generated 5 recommendations
- Found 1 intermittent failure

ACTION REQUIRED:
- Add findings to test_recommendations.md
- Update test coverage section in TESTING_SUMMARY.md
- Document known issues in test_results.md

ARTIFACTS:
- Analysis report (inline above)
- Recommended test cases (list attached)
```

## Commands for Analysis

```bash
# Read test results
cat automated_test_results.json | python -m json.tool

# Compare results
diff automated_test_results_previous.json automated_test_results.json

# Check logs
tail -100 agent_debug.log | grep -i "error\|fail\|warning"

# Analyze test workspace
ls -lh test_workspace/
du -sh test_workspace/
```

---

**Remember:** Analysis without recommendations is just data. Always provide actionable insights.
